{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing linear regression with NumPy\n",
    "\n",
    "In this notebook we implement linear regression with NumPy, to train models on the on the Portland housing dataset.\n",
    "\n",
    "The following exercises need to be completed for week 5.\n",
    "\n",
    "\n",
    "\n",
    "**Beware** The changes that you make to this on-line notebook will not be saved. As soon as you close or lose this tab, your answers will disappear.\n",
    "\n",
    "**Download this notebook to you local disk regularly**  \n",
    "\n",
    "**Bring a copy** of the finished exercises to the next class. Your notebook will be graded by the teacher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The portland dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Portlant housing data\n",
    "\n",
    "Run the cells below to load the Portland dataset.  \n",
    "*Hint: press shift+enter to evaluate cells.  \n",
    "The menu Help -> Keyboard shortcuts lists more shortcuts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "sns.set_context(\"poster\")\n",
    "plt.style.use('fivethirtyeight')\n",
    "matplotlib.style.use('fivethirtyeight') \n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "scatter_size = 60\n",
    "\n",
    "portland = pd.read_csv('portlant_housing.txt', header=None, names=['area', 'bedrooms', 'price'])\n",
    "portland.price = (portland.price / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array and DataFrame\n",
    "The variable `portland` is a so called Pandas `DataFrame`. Pandas is a module for working with structured data. Don't worry if you are not familiar with about Pandas. Pandas provides extra functionality on top of NumPy. Behind every pandas `DataFrame` lies a NumPy `ndarray`. `Dataframe` has some methods in common with `ndarray`. Furthermore, NumPy functions accept `DataFrame` type arguments.\n",
    "\n",
    "\n",
    "1. Get the type of the variable portland\n",
    "1. Get a listing of the first 10 houses by making use of `portland.head()`.  \n",
    " *Hint: use the help function to learn more about df.head()*\n",
    "5. Extract the underlying NumPy array of `portland` by calling  `portland.values`, and slice the array to show the first 5 rows only. Use brackets `[]` for the slicing.  \n",
    "*Hint: remember the values attribute for later exercises where you need a NumPy `ndarray` instead of a Pandas DataFrame.*\n",
    "5. Subsetting works slightly different with Pandas. Use 'portland.iloc' to get the last three rows of the dataframe.  \n",
    "*Hint: use the help function on 'portland.iloc'*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Portland dataset\n",
    "\n",
    "Subsetting works similar on `DataFrames`\n",
    "\n",
    "1. How many data points are in the DataFrame `portland`? Use `portland.shape`.\n",
    "2. What are the mean, minimum and maximum of the number of bedrooms and of the price? Use `portland.mean()`, `portland.max()` and `portland.min()`.\n",
    "3. Make a scatter plot of the dataset by using the code block below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,4),dpi=100)\n",
    "ax0 = plt.subplot(121)\n",
    "portland.plot('area', 'price', kind='scatter', ax=ax0, s=scatter_size);\n",
    "plt.xlabel(\"Size of house (x_1)\")\n",
    "plt.ylabel(\"Price (Y)\")\n",
    "ax1 = plt.subplot(122)\n",
    "portland.plot('bedrooms', 'price', kind='scatter', ax=ax1, s=scatter_size);\n",
    "plt.xlabel(\"Number of Bedrooms (X_2)\")\n",
    "plt.ylabel(\"Price (Y)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Square loss with pure Python\n",
    "1. Use pure python to write mean square loss function. Do not use NumPy or any other library for the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss_python(h, y):\n",
    "    \"\"\"\n",
    "    Returns the mean square loss of h and y\n",
    "    h: list of floats\n",
    "    y: list of floats\n",
    "    return: float\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Check answer\n",
    "square_loss_python([3., 4., 5.], [2., 1, 1]) == 26/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/square_loss_python.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Square loss with Numpy\n",
    "2. Use `numpy.dot` to write the mean square loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(h, y):\n",
    "    \"\"\"\n",
    "    Returns the mean square loss of h and y\n",
    "    h: numpy vector\n",
    "    y: numpy float\n",
    "    return: float\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# Check answer\n",
    "square_loss(np.array([3., 4., 5.]), np.array([2., 1, 1])) == 26/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/square_loss.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple hypothesis\n",
    "\n",
    "1. The simplest hypothesis we can come up with, is \n",
    "$$\n",
    "h_0(x) = \\theta_0 \\,,\n",
    "$$\n",
    "a constant. What would be a reasonable value for $\\theta$?\n",
    "2. Estimate the mean square loss for this constant hypothesis, based on the plots above. Explain you reasoning.\n",
    "2. Create a vector $h$ for the Portland dataset, based on your hypothesis.  \n",
    "*Hint: make use of use of `np.ones', np.ones_like`.\n",
    "3. Use the `square_loss()` to calculate the loss of $h_0$ on the Portland dataset\n",
    "4. Consider the set of constant hypothesis,\n",
    "$$\n",
    "\\mathcal H_\\text{constant} = \\left\\{h_\\theta(\\mathbf x) = \\theta_0\\,, \\theta_0 \\in \\mathbb R \\right\\} \\,.\n",
    "$$\n",
    "Under these Hypothesis, the loss function becomes\n",
    "$$\n",
    "J(\\theta_0) = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\theta_0 )^2 \\,.\n",
    "$$\n",
    "We can find the analytical minimum of the cost function, by setting the derivative with respect to $\\theta_0$ to zero. \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial \\theta_0}  J(\\theta_0) & = \\frac{2}{N} \\sum_{i=1}^N( y_i - \\theta_0)  \\\\\n",
    "& = \\frac{2}{N} \\sum_{i=1}^N y_i - 2 \\theta_0  \\\\\n",
    "& = 0  \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Solve this equation for $\\theta_0$.\n",
    "Explain the solution in words\n",
    "6. Use NumPy to compute the optimal value for $\\theta_0$ for the Portland dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/constant_hypothesis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Square loss function with TensorFlow\n",
    "\n",
    "1. Complete the function 'square_loss_tensor' to calculate the loss with TensorFlow. The expected output is an abstract TensorFlow array. In other words: you do not have to evaluate the loss within this function. \n",
    "2. Use TensorFlow to evaluate the mean square loss for your constant hypothesis in the last exercise. Add comments in you code, and include the following parts\n",
    "    - Python variables\n",
    "    - Setup a computation Graph\n",
    "    - Create place holders\n",
    "    - Use your function `square_loss_tensor`\n",
    "    - Create a session\n",
    "    - Perform the calculation\n",
    "    - Present the result  \n",
    "You do not have to wrap your solution into a function.\n",
    "3. Compare the loss of the TensorFlow implementation with the Python implementation. Are they identical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss_tensor(h, y):\n",
    "    \"\"\"\n",
    "    Construct a node that computes the mean square loss as function of nodes h and y.\n",
    "\n",
    "    Inputs:\n",
    "    - h: TensorFlow tensor of dimension N with predicted outcomes for y\n",
    "    - y: TensorFlow tensor of dimension N with actual outcomes for y\n",
    "\n",
    "    Returns: TensorFlow scalar of type float \n",
    "        this is an abstract tensor of rank 0\n",
    "    \"\"\"\n",
    "    return tf.constant(0) # Replace this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/square_loss_tensor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check answer\n",
    "h_tensor = tf.constant([3., 4., 5.], name=\"h\")\n",
    "y_tensor = tf.constant([2., 1, 1], name=\"y\")\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(square_loss_tensor(h_tensor, y_tensor)))\n",
    "    # should be 8.666...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design matrix\n",
    "\n",
    "In linear regression, we work with the set of hypothesis \n",
    "\n",
    "$$ \n",
    "\\mathbf h_\\theta (X) = X \\theta\n",
    "$$\n",
    "\n",
    "1. What is in the rows of $X$?\n",
    "2. What is in the columns of $X$?\n",
    "2. What is the bias term? And why is it missing in the Hypothesis?\n",
    "2. Run the two cells below. Explain how it adds a bias term to the Hypothesis. What happens with the column price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "portland.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_matrix = portland.copy()\n",
    "design_matrix['bias'] = 1\n",
    "design_matrix = design_matrix.drop('price', axis=1)\n",
    "design_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Hypothesis\n",
    "Above, you have defined a constant hypothesis. Now it is time to take the independent variables into account. \n",
    "\n",
    "\n",
    "1.1. What are the independent variables / features in this problem?\n",
    "1. What would be a reasonable guess for theta, when you take all independent variables into account?\n",
    "1. Adapt the NumPy vector `theta_test` with your guess for $\\mathbf \\theta$.\n",
    "1. Implement the linear hypothesis by completing the function `linear_regession_hypothesis`  \n",
    "*Hint: you may want to check the Numpy documentation to find the function you need*\n",
    "1. Apply the function `linear_regession_hypothesis` to the Portland dataset at `theta_test`\n",
    "1. Does your regression implementation gives a reasonable outcomes?\n",
    "1. Calculate the errors with `error = linear_hypothesis(X, theta_test) - y`. What is the mean of the errors? Do you see reason to adapt your hypothesis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_test = np.array([0, 0, 0])\n",
    "X = design_matrix.values\n",
    "y = portland.price.values\n",
    "\n",
    "def linear_hypothesis(X_batch, theta):\n",
    "    \"\"\"\n",
    "    Use the weights theta to predict outcomes for the data points X_batch.\n",
    "\n",
    "    Inputs:\n",
    "    - X_batch: A numpy array of shape (N, p) containing a minibatch of N\n",
    "      data points; each point has dimension p.\n",
    "    - theta: A numpy array of shape (p,) containing the weights of the hypothesis\n",
    "\n",
    "    Returns: A tuple containing:\n",
    "    - loss as a single float\n",
    "    - gradient with respect to the parameter vector \\theta\n",
    "    \n",
    "    Returns:\n",
    "    - y_pred: Predicted outcomes for the data in X. y_pred is a 1-dimensional\n",
    "      array of length N\n",
    "    \"\"\"\n",
    "    \n",
    "linear_hypothesis(X, theta_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/linear_hypothesis.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update rules for linear regression\n",
    "\n",
    "The update rules for gradient decent are\n",
    "\n",
    "$$\\mathbf \\theta_j := \\mathbf \\theta_{j-1} - \\alpha \\left. \\nabla_\\theta J(\\theta) \\right\\rvert_{\\theta = \\theta_{j-1}}\\,,$$\n",
    "where $J(\\mathbf \\theta)$ is the loss function. The exact from of the update rules depend on the model and the loss function that is used. The gradient of the mean square error for the linear hypothesis above equals\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\frac{2}{N} \\mathbf X^\\top (\\mathbf h_{\\theta} - \\mathbf y) \\,.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "1. What is the meaning of the index $j$?\n",
    "2. What are the dimensions of $\\mathbf X^\\top$, $\\mathbf y$ and $\\mathbf \\theta$?\n",
    "4. What is the interpretation of $\\mathbf h_{\\theta_{j-1}} - \\mathbf y$?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for linear regression\n",
    "\n",
    "You are going to implement the gradients for linear regression using NumPy. The function `linear_regression_gradient` has been provided below.\n",
    "\n",
    "2. Finish `linear_regression_gradient`\n",
    "    1. For now you can ignore regularization parameter `reg`\n",
    "    1. Make use of your function `linear_hypothesis` the calculate the predictions\n",
    "    1. Make use of your function `square_loss(h, y)` to calculate the loss\n",
    "    1. Make use of `X_batch.transpose()` and `np.matmul` to calculate the gradient. \n",
    "3. Compute the gradient for the Portland dataset with $\\theta$ = `theta_test`, which you defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_gradient(X_batch, y_batch, theta, reg=0):\n",
    "    \"\"\"\n",
    "    Compute the loss function and its derivative. \n",
    "\n",
    "    Inputs:\n",
    "    - X_batch: A numpy array of shape (N, p) containing a minibatch of N\n",
    "      data points; each point has dimension p.\n",
    "    - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
    "    - theta: A numpy array of shape (p) of model parameters\n",
    "    - reg: regularization strength\n",
    "\n",
    "    Returns: A tuple containing:\n",
    "    - loss as a single float\n",
    "    - gradient with respect to the parameter vector \\theta\n",
    "    \"\"\"\n",
    "    loss = None\n",
    "    gradient = None\n",
    "    return (loss, gradient)\n",
    "\n",
    "loss, gradient = linear_regression_gradient(X, y, np.array([0,0,0])\n",
    "print(loss) # 131183.09621291485\n",
    "print(gradient) # [-1.52841826e+06, -2.24073540e+03, -6.80825319e+02]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/linear_regression_gradient.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_gradient(X, y, theta_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "1. Looking at the order of magnitude of the gradients for `theta_test`, what do you observe?\n",
    "1. Why do we need to scale the design matrix?\n",
    "1. Run the code below to scale the design matrix\n",
    "1. Evaluate the gradient again, now on with `theta_test_scaled`, which is given below.\n",
    "1. Do you still see a problem with the gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_matrix_scaled = (design_matrix - design_matrix.mean())/(design_matrix.std())\n",
    "design_matrix_scaled.bias = 1\n",
    "design_matrix_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_test_scaled = [110, -5, 340]\n",
    "linear_regression_gradient(design_matrix_scaled.values, portland.price.values, theta_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing gradient descent\n",
    "Below you can find code for the class `Regression`. \n",
    "1. What happens in the __init__() method?\n",
    "1. How does `theta` gets initialized, eventually?\n",
    "1. Complete the method `predict`\n",
    "   1. Use `linear_hypothesis` that you have written before\n",
    "   1. For `theta`, use `self.theta`\n",
    "1. Complete the method `train` by updating the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "\n",
    "    def __init__(self, reg=0):\n",
    "        self.theta = None\n",
    "        self.loss_history = []\n",
    "        self.reg = reg\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, num_iters=100,\n",
    "        batch_size=None, verbose=False):\n",
    "        \"\"\"\n",
    "        Train this linear model using stochastic gradient descent.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, p) containing training data; there are N\n",
    "          training samples each of dimension p.\n",
    "        - y: A numpy array of shape (N,) containing training labels\n",
    "        - learning_rate: (float) learning rate for optimization.\n",
    "        - reg: (float) regularization strength.\n",
    "        - num_iters: (integer) number of steps to take when optimizing\n",
    "        - batch_size: (integer) number of training examples to use at each step.\n",
    "        - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "        Outputs:\n",
    "        A list containing the value of the loss function at each training iteration.\n",
    "        \"\"\"\n",
    "        num_train, p = X.shape\n",
    "        if self.theta is None:\n",
    "          # initialize weight vector theta with random values\n",
    "          self.theta = 0.001 * np.random.randn(p)\n",
    "\n",
    "        # Stochastic gradient descent to optimize theta\n",
    "        loss_history = []\n",
    "        for it in range(num_iters): # training iterations\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "            if batch_size:\n",
    "                  batch = np.random.choice(len(X), batch_size)\n",
    "                  X_batch = X[batch, :]\n",
    "                  y_batch = y[batch]\n",
    "            else:\n",
    "                X_batch = X\n",
    "                y_batch = y\n",
    "            \n",
    "              # Evaluate loss and gradient\n",
    "            loss, grad = self.loss(X_batch, y_batch, self.reg)\n",
    "            self.loss_history.append(loss)\n",
    "\n",
    "            # perform parameter update\n",
    "            #########################################################################\n",
    "            # TODO:                                                                 #\n",
    "            # Update the weights using the gradient and the learning rate.          #\n",
    "            #########################################################################\n",
    "            \n",
    "            #########################################################################\n",
    "            #                       END OF YOUR CODE                                #\n",
    "            #########################################################################\n",
    "\n",
    "            if verbose and (it + 1) % 20 == 0:\n",
    "                print('iteration %d / %d: loss %f' % (it+1, num_iters, loss))\n",
    "                #print('theta', self.theta)\n",
    "                #print('grad', grad)\n",
    "                #print('upate', -learning_rate * grad)\n",
    "\n",
    "          \n",
    "        return \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this linear model to predict labels for\n",
    "        data points.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, p) containing training data; there are N\n",
    "          training samples each of dimension p.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: Predicted values for the data in X. y_pred is a 1-dimensional\n",
    "          array of length N\n",
    "        \"\"\"\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        # Make prediction\n",
    "        #########################################################################\n",
    "        # TODO:                                                                 #\n",
    "        # Use the learned coefficients to make a prediction                     #\n",
    "        #########################################################################\n",
    "        \n",
    "        #########################################################################\n",
    "        #                       END OF YOUR CODE                                #\n",
    "        #########################################################################\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        \"\"\"\n",
    "        Compute the loss function and its derivative. \n",
    "\n",
    "        Inputs:\n",
    "        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
    "          data points; each point has dimension D.\n",
    "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
    "        - reg: (float) regularization strength.\n",
    "\n",
    "        Returns: A tuple containing:\n",
    "        - loss as a single float\n",
    "        - gradient with respect to self.W; an array of the same shape as W\n",
    "        \"\"\"\n",
    "        loss, grad = linear_regression_gradient(X_batch, y_batch, self.theta, reg)\n",
    "        return (loss, grad)\n",
    "\n",
    "    def plot_loss(self):\n",
    "        if self.loss_history:\n",
    "            plt.plot(self.loss_history)\n",
    "            plt.title('Learning curve')\n",
    "            plt.xlabel('Mini batches')\n",
    "            plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/linear_regression_class.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the learning rate\n",
    "\n",
    "After completing the class 'LinearRegression', you can train it on the Portland dataset.\n",
    "1. Run the code below to train the model an view the learning curve\n",
    "2. Adapt the learning rate and the number of iterations to bring down the loss. Keep record of your trials.\n",
    "3. What are the coefficients that are learned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = LinearRegression()\n",
    "regression.train(design_matrix_scaled.values, portland.price.values, batch_size=None, learning_rate=1e-2, num_iters=100, verbose=True)\n",
    "regression.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/training_portland.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
