{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes implementing linear regression with NumPy\n",
    "\n",
    "In this notebook we implement linear regression for the Portland housing dataset.\n",
    "\n",
    "The following exercises need to be completed for week 4.\n",
    "\n",
    "\n",
    "\n",
    "**Beware** The changes that you make to this on-line notebook will not be saved. As soon as you close this tab, your answers will disappear.\n",
    "\n",
    "**Download this notebook to you local disk regularly**  \n",
    "\n",
    "**Bing a copy** of the finished exercises to the next class. Your notebook will be graded by the teacher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The portland dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Portlant housing data\n",
    "\n",
    "Run the cells below to load the Portland dataset.  \n",
    "*Hint: press shift+enter to evaluate cells.  \n",
    "The menu Help -> Keyboard shortcuts lists more shortcuts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "sns.set_context(\"poster\")\n",
    "plt.style.use('fivethirtyeight')\n",
    "matplotlib.style.use('fivethirtyeight') \n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "scatter_size = 60\n",
    "\n",
    "portland = pd.read_csv('portlant_housing.txt', header=None, names=['area', 'bedrooms', 'price'])\n",
    "portland.price = (portland.price / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Array and DataFrame\n",
    "The variable `portland` is a so called Pandas `DataFrame`. Pandas is a module for working with structured data. Don't worry if you are not familiar with about Pandas. Pandas provides extra functionality on top of NumPy. Behind every pandas `DataFrame` lies a NumPy `ndarray`. `Dataframe` has some methods in common with `ndarray`. Furthermore, NumPy functions accept `DataFrame` type arguments.\n",
    "\n",
    "\n",
    "1. Get the type of the variable portland\n",
    "1. Get a listing of the first 10 houses by making use of `portland.head()`.  \n",
    "   *Hint: use the help function to learn more about df.head()*\n",
    "2. Give an example of a NumPy function  \n",
    "*Hint: go to menu Help -> NunPy*\n",
    "3. Give an example of a Numpy array method  \n",
    "*Hint: go to menu Help -> NunPy*\n",
    "4. Give an example of a NumPy `ndarray` attribute  \n",
    "*Hint: go to menu Help -> NunPy*\n",
    "5. Subsetting works slightly different with Pandas. Use 'portland.iloc' to get the last three rows of the dataframe.  \n",
    "*Hint: use the help function on 'portland.iloc'*\n",
    "5. Extract the underlying NumPy array of `portland` by making use of the `values` attribute and slice the array to show the first 5 rows only. \n",
    "*Hint: remember the values attribute for later exercises where you need a NumPy `ndarray` instead of a Pandas DataFrame*\n",
    "8. Get the type of the NumPy array behind the variable `portland`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the Portland dataset\n",
    "\n",
    "Subsetting works similar on `DataFrames`\n",
    "\n",
    "1. How many data points are in the dataframe `portland`? Use a `DataFrame` or Python function to get to the answer. Don't count manually.\n",
    "2. What are the mean, minimum and maximum of the number of bedrooms and of the price? Use methods of `DataFrame` to calculate this.\n",
    "3. Plot the dataset using the code block below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,4),dpi=100)\n",
    "ax0 = plt.subplot(121)\n",
    "portland.plot('price', 'area', kind='scatter', ax=ax0, s=scatter_size);\n",
    "plt.xlabel(\"Size of house (x_1)\")\n",
    "plt.ylabel(\"Price (Y)\")\n",
    "ax1 = plt.subplot(122)\n",
    "portland.plot('bedrooms', 'price', kind='scatter', ax=ax1, s=scatter_size);\n",
    "plt.xlabel(\"Number of Bedrooms (X_2)\")\n",
    "plt.ylabel(\"Price (Y)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Square loss with pure Python\n",
    "1. Use pure python to write mean square loss function. Do not use NumPy or any other library for the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss_python(h, y):\n",
    "    \"\"\"\n",
    "    Returns the mean square loss of h and y\n",
    "    h: list of floats\n",
    "    y: list of floats\n",
    "    return: float\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Square loss with Numpy\n",
    "2. Use pure python to write mean square loss function. Do not use numpy or any other library for the calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss(h, y):\n",
    "    \"\"\"\n",
    "    Returns the mean square loss of h and y\n",
    "    h: numpy vector\n",
    "    y: numpy float\n",
    "    return: float\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple hypothesis\n",
    "\n",
    "1. Come up with a simple hypothesis for housing price.  \n",
    "Express your hypothesis in the form\n",
    "$$\n",
    "h(x) = \\cdots\n",
    "$$\n",
    "*Hint: the simplest hypothesis you can think of for a regression problem, is a constant number*.\n",
    "2. Estimate the mean square loss for your hypothesis. Explain your reasoning.\n",
    "2. Create a vector $h$ for the Portland dataset, based on your hypothesis.  \n",
    "*Hint: use `np.ones_like`*\n",
    "3. Use the Portland dataset to calculate the loss for your hypothesis. Use both implementations of the loss functions to check if they give the same result.\n",
    "4. Consider the set of constant hypothesis,\n",
    "$$\n",
    "\\mathcal H_\\text{constant} = \\left\\{h_\\theta(\\mathbf x) = \\theta_0\\,, \\theta_0 \\in \\mathbb R \\right\\} \\,.\n",
    "$$\n",
    "How can you minimize the square loss with respect to this set of Hypothesis? In other words, what is the analytical minimum for this set of hypothesis? Explain your answer.\n",
    "6. Use NumPy to compute the optimal value for $\\theta_0$ for the Portland dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Square loss function with TensorFlow\n",
    "\n",
    "1. Complete the function 'square_loss_tensor' to calculate the loss with TensorFlow. The expected output is an abstract TensorFlow array. In other words: you do not have to evaluate the loss within this function. \n",
    "2. Use TensorFlow to evaluate the mean square loss for your constant hypothesis in the last exercise. Add comments in you code, and include the following parts\n",
    "    - Python variables\n",
    "    - Setup a computation Graph\n",
    "    - Create place holders\n",
    "    - Use your function `square_loss_tensor`\n",
    "    - Create a session\n",
    "    - Perform the calculation\n",
    "    - Present the result  \n",
    "You do not have to wrap your solution into a function.\n",
    "3. Compare the loss of the TensorFlow implementation with the Python implementation. Are they identical?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_loss_tensor(h, y):\n",
    "    \"\"\"\n",
    "    Construct a node that computes the mean square loss.\n",
    "\n",
    "    Inputs:\n",
    "    - h: TensorFlow tensor of dimension N with predicted outcomes for y\n",
    "    - y: TensorFlow tensor of dimension N with actual outcomes for y\n",
    "\n",
    "    Returns: TensorFlow scalar float with the mean loss\n",
    "        this is an abstract tensor of rank 0\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Design matrix\n",
    "\n",
    "In linear regression, we work with the set of hypothesis \n",
    "\n",
    "$$ \n",
    "\\mathbf h_\\theta (X) = X \\theta\n",
    "$$\n",
    "\n",
    "1. What is in the rows of $X$?\n",
    "2. What is in the columns of $X$?\n",
    "2. What is the bias term? And why is it missing in the Hypothesis?\n",
    "2. Run the two cells below. Explain how it adds a bias term to the Hypothesis. What happens with the column price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portland.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_matrix = portland\n",
    "design_matrix['bias'] = 1\n",
    "design_matrix = design_matrix.drop('price', axis=1)\n",
    "design_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Hypothesis\n",
    "Above, you have defined a constant hypothesis. Now it is time to take the independent variables into account. \n",
    "\n",
    "\n",
    "1. What is another name for independent variables?\n",
    "1. What are the independent variables in this problem?\n",
    "1. What would be a reasonable guess for theta, when you take all independent variables into account?\n",
    "1. Create a NumPy vector `theta_test` with reasonable values for $\\mathbf \\theta$\n",
    "1. Implement the linear hypthesis by completing the function `linear_regession_hypothesis`  \n",
    "*Hint: you may want to check the Numpy documentation to find the function you need*\n",
    "1. Apply the function `linear_regession_hypothesis` to the Portland dataset at `theta_test`\n",
    "1. Does your regression implementation gives a reasonable outcome? Explain  \n",
    "*Hint: take the difference between the predicted price and the real price*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_hypothesis(X_batch, theta):\n",
    "    \"\"\"\n",
    "    Use the weights theta to predict outcomes for the data points X_batch.\n",
    "\n",
    "    Inputs:\n",
    "    - X_batch: A numpy array of shape (N, p) containing a minibatch of N\n",
    "      data points; each point has dimension p.\n",
    "    - theta: A numpy array of shape (p,) containing the weights of the hypothesis\n",
    "\n",
    "    Returns: A tuple containing:\n",
    "    - loss as a single float\n",
    "    - gradient with respect to the parameter vector \\theta\n",
    "    \n",
    "    Returns:\n",
    "    - y_pred: Predicted outcomes for the data in X. y_pred is a 1-dimensional\n",
    "      array of length N\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update rules for linear regression\n",
    "\n",
    "The update rules for gradient decent are\n",
    "\n",
    "$$\\mathbf \\theta_j := \\mathbf \\theta_{j-1} - \\alpha \\left. \\nabla_\\theta J(\\theta) \\right\\rvert_{\\theta = \\theta_{j-1}}\\,,$$\n",
    "where $J(\\mathbf \\theta)$ is the loss function. The exact from of the update rules depend on the model and the loss function that is used. The gradient of the mean square error for the linear hypothesis above cam be shown to equal\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta J(\\theta) = \\frac{2}{N} \\mathbf X^\\top \\cdot (\\mathbf h_{\\theta} - \\mathbf y) \\,.\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "1. What is the meaning of the index $j$?\n",
    "2. What are the dimensions of $\\mathbf X^\\top$, $\\mathbf y$ and $\\mathbf \\theta$?\n",
    "3. What does the inner product sum over?\n",
    "4. What is the interpertation of $\\mathbf h_{\\theta_{j-1}} - \\mathbf y$?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients for linear regression\n",
    "\n",
    "You are going to implement the gradients for linear regression using NumPy, for a batch of data. The function `linear_regression_gradient` has been provided below. For rest of the assignments to work, it is important that you  follow the definitions in the function description closely.\n",
    "\n",
    "1. What is a Python tuple?\n",
    "2. Finish `linear_regression_gradient`\n",
    "    1. Make use of your function `linear_hypothesis`\n",
    "    1. Make use of your function `square_loss(h, y)`\n",
    "3. Compute the gradient for the portland dataset with $\\theta$ = `theta_test`, which you defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_gradient(X_batch, y_batch, theta):\n",
    "    \"\"\"\n",
    "    Compute the loss function and its derivative. \n",
    "\n",
    "    Inputs:\n",
    "    - X_batch: A numpy array of shape (N, p) containing a minibatch of N\n",
    "      data points; each point has dimension p.\n",
    "    - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
    "\n",
    "    Returns: A tuple containing:\n",
    "    - loss as a single float\n",
    "    - gradient with respect to the parameter vector \\theta\n",
    "    \"\"\"\n",
    "    loss = None\n",
    "    gradient = None\n",
    "    return (loss, gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "1. Looking at the gradients for `theta_test`, what do you observe?\n",
    "1. Why do we need to scale the design matrix?\n",
    "1. Run the code below to scale the design matrix\n",
    "1. Evaluate the gradient again, now on with `theta_test_scaled`, which is given below.\n",
    "1. Do you still see a problem with the gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_matrix_scaled = (design_matrix - design_matrix.mean())/(design_matrix.std())\n",
    "design_matrix_scaled.bias = 1\n",
    "design_matrix_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_test_scaled = [110, -5, 340]\n",
    "linear_regression_gradient(design_matrix_scaled.values, portland.price.values, theta_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing gradient descent\n",
    "Below you can find code for the class `Regression`. \n",
    "1. What happens in the __init__() method?\n",
    "1. How does `theta` gets initialized, eventually?\n",
    "1. Complete the method `predict`  \n",
    "*Hint: within the method 'predict', you can call the function `linear_hypothesis` that you have written before*\n",
    "1. Complete the method `train` by updating the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "        self.loss_history = []\n",
    "\n",
    "    def train(self, X, y, learning_rate=1e-3, num_iters=100,\n",
    "        batch_size=None, verbose=False):\n",
    "        \"\"\"\n",
    "        Train this linear model using stochastic gradient descent.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, p) containing training data; there are N\n",
    "          training samples each of dimension p.\n",
    "        - y: A numpy array of shape (N,) containing training labels\n",
    "        - learning_rate: (float) learning rate for optimization.\n",
    "        - reg: (float) regularization strength.\n",
    "        - num_iters: (integer) number of steps to take when optimizing\n",
    "        - batch_size: (integer) number of training examples to use at each step.\n",
    "        - verbose: (boolean) If true, print progress during optimization.\n",
    "\n",
    "        Outputs:\n",
    "        A list containing the value of the loss function at each training iteration.\n",
    "        \"\"\"\n",
    "        num_train, p = X.shape\n",
    "        if self.theta is None:\n",
    "          # lazily initialize theta\n",
    "          self.theta = 0.001 * np.random.randn(p)\n",
    "\n",
    "        # Run stochastic gradient descent to optimize W\n",
    "        loss_history = []\n",
    "        for it in range(num_iters):\n",
    "            X_batch = None\n",
    "            y_batch = None\n",
    "            if batch_size:\n",
    "                  batch = np.random.choice(len(X), batch_size)\n",
    "                  X_batch = X[batch, :]\n",
    "                  y_batch = y[batch]\n",
    "            else:\n",
    "                X_batch = X\n",
    "                y_batch = y\n",
    "            \n",
    "              # evaluate loss and gradient\n",
    "            loss, grad = self.loss(X_batch, y_batch, reg)\n",
    "            self.loss_history.append(loss)\n",
    "\n",
    "              # perform parameter update\n",
    "              #########################################################################\n",
    "              # TODO:                                                                 #\n",
    "              # Update the weights using the gradient and the learning rate.          #\n",
    "              #########################################################################\n",
    "\n",
    "              #########################################################################\n",
    "              #                       END OF YOUR CODE                                #\n",
    "              #########################################################################\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "                #print('theta', self.theta)\n",
    "                #print('grad', grad)\n",
    "                #print('upate', -learning_rate * grad)\n",
    "\n",
    "          \n",
    "        return \n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Use the trained weights of this linear model to predict labels for\n",
    "        data points.\n",
    "\n",
    "        Inputs:\n",
    "        - X: A numpy array of shape (N, p) containing training data; there are N\n",
    "          training samples each of dimension p.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: Predicted values for the data in X. y_pred is a 1-dimensional\n",
    "          array of length N\n",
    "        \"\"\"\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "        # Make prediction\n",
    "        #########################################################################\n",
    "        # TODO:                                                                 #\n",
    "        # Use the learned coefficients to make a prediction                     #\n",
    "        #########################################################################\n",
    "\n",
    "        #########################################################################\n",
    "        #                       END OF YOUR CODE                                #\n",
    "        #########################################################################\n",
    "        \n",
    "        return y_pred\n",
    "\n",
    "    def loss(self, X_batch, y_batch, reg):\n",
    "        \"\"\"\n",
    "        Compute the loss function and its derivative. \n",
    "        Subclasses will override this.\n",
    "\n",
    "        Inputs:\n",
    "        - X_batch: A numpy array of shape (N, D) containing a minibatch of N\n",
    "          data points; each point has dimension D.\n",
    "        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.\n",
    "        - reg: (float) regularization strength.\n",
    "\n",
    "        Returns: A tuple containing:\n",
    "        - loss as a single float\n",
    "        - gradient with respect to self.W; an array of the same shape as W\n",
    "        \"\"\"\n",
    "        loss, grad = linear_regression_gradient(X_batch, y_batch, self.theta)\n",
    "        return (loss, grad)\n",
    "\n",
    "    def plot_loss(self):\n",
    "        if self.loss_history:\n",
    "            plt.plot(self.loss_history)\n",
    "            plt.title('Learning curve')\n",
    "            plt.xlabel('Mini batches')\n",
    "            plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the learning rate\n",
    "\n",
    "After completing the class 'LinearRegression', you can train it on the porltland dataset.\n",
    "1. Run the code below to train the model an view the learning curve\n",
    "2. Adapt the learning rate and the number of iterations to bring down the loss. Keep record of your trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = LinearRegression()\n",
    "regression.train(design_matrix_scaled.values, portland.price.values, batch_size=None, learning_rate=1e-1, num_iters=100, verbose=True)\n",
    "regression.plot_loss()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
